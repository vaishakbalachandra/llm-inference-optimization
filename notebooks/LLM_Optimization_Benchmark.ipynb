{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLz35ypRBeSi"
      },
      "source": [
        "# Multi-Model LLM Inference Optimization\n",
        "**Author:** Vaishak Balachandra  \n",
        "**University:** Purdue University - MS Computer Science  \n",
        "**Project:** Comprehensive LLM optimization across model sizes\n",
        "\n",
        "---\n",
        "\n",
        "## Models:\n",
        "1. GPT-2 (124M)\n",
        "2. Mistral-7B (7B)\n",
        "3. Llama-2-7B (7B) - Optional\n",
        "\n",
        "## Optimizations:\n",
        "1. FlashAttention-2\n",
        "2. 4-bit Quantization\n",
        "3. FP16 Precision\n",
        "\n",
        "**Hardware:** A100 GPU  \n",
        "**Runtime:** ~60 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOXHKuzABeSl"
      },
      "outputs": [],
      "source": [
        "# GPU Verification\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"SYSTEM VERIFICATION\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
        "print(f\"âœ… CUDA: {torch.cuda.is_available()}\")\n",
        "print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "if any(x in gpu_name for x in ['A100', 'A10', 'H100', 'L4']):\n",
        "    print(f\"\\nâœ… {gpu_name} supports FlashAttention-2!\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ {gpu_name} may not support FlashAttention-2\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOMneUX3BeSm"
      },
      "source": [
        "## HuggingFace Authentication (Optional - for Llama-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivFJzfSrBeSn"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"ðŸ” HuggingFace Authentication (for Llama-2)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "use_llama = input(\"Include Llama-2? (y/n): \").lower().strip()\n",
        "\n",
        "LLAMA_AVAILABLE = False\n",
        "\n",
        "if use_llama == 'y':\n",
        "    try:\n",
        "        hf_token = getpass(\"Paste your HF token (hidden): \")\n",
        "        login(token=hf_token, add_to_git_credential=False)\n",
        "\n",
        "        # Verify access\n",
        "        from huggingface_hub import list_repo_files\n",
        "        files = list_repo_files(\"meta-llama/Llama-2-7b-hf\", token=hf_token)\n",
        "\n",
        "        print(\"\\nâœ… Authenticated! Llama-2 will be included\")\n",
        "        LLAMA_AVAILABLE = True\n",
        "\n",
        "        # Store token in environment\n",
        "        import os\n",
        "        os.environ['HF_TOKEN'] = hf_token\n",
        "        os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Authentication failed: {e}\")\n",
        "        print(\"Continuing without Llama-2\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Skipping Llama-2\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Models: GPT-2, Mistral-7B\" + (\", Llama-2-7B\" if LLAMA_AVAILABLE else \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3eTa4W0BeSn"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "print(\"ðŸ“¦ Installing packages (5-8 minutes)...\\n\")\n",
        "\n",
        "!pip install -q transformers accelerate bitsandbytes sentencepiece protobuf\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "print(\"\\nâœ… Installation complete!\")\n",
        "\n",
        "import transformers\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "\n",
        "print(f\"âœ… transformers: {transformers.__version__}\")\n",
        "print(f\"âœ… accelerate: {accelerate.__version__}\")\n",
        "print(f\"âœ… bitsandbytes: {bitsandbytes.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Bw7F_rBeSn"
      },
      "outputs": [],
      "source": [
        "# Benchmarking Framework\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "import pandas as pd\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ALL_RESULTS = []\n",
        "\n",
        "def benchmark_model(model, tokenizer, model_name, config_name,\n",
        "                   prompt=\"Explain quantum computing:\",\n",
        "                   num_runs=8, max_new_tokens=100):\n",
        "    device = next(model.parameters()).device\n",
        "    times = []\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    input_len = inputs.input_ids.shape[1]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ”¬ BENCHMARKING: {model_name} - {config_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"ðŸ“ Prompt: '{prompt[:60]}...'\")\n",
        "    print(f\"ðŸŽ¯ Runs: {num_runs} + 1 warmup\\n\")\n",
        "\n",
        "    # Warmup\n",
        "    print(\"ðŸ”¥ Warmup...\", end=\" \")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            _ = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id\n",
        "            )\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        print(\"âœ…\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Benchmark\n",
        "    for i in range(num_runs):\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            end = time.time()\n",
        "\n",
        "            elapsed = end - start\n",
        "            tokens_gen = outputs.shape[1] - input_len\n",
        "            times.append(elapsed)\n",
        "\n",
        "            print(f\"ðŸ“Š Run {i+1:2d}: {elapsed:.3f}s ({tokens_gen} tok, {tokens_gen/elapsed:.1f} tok/s)\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Run {i+1} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    if len(times) == 0:\n",
        "        return None\n",
        "\n",
        "    times = np.array(times)\n",
        "    throughputs = max_new_tokens / times\n",
        "\n",
        "    try:\n",
        "        memory_gb = model.get_memory_footprint() / 1e9\n",
        "    except:\n",
        "        memory_gb = torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "    results = {\n",
        "        'model': model_name,\n",
        "        'config': config_name,\n",
        "        'mean_time': float(np.mean(times)),\n",
        "        'std_time': float(np.std(times)),\n",
        "        'median_throughput': float(np.median(throughputs)),\n",
        "        'mean_throughput': float(np.mean(throughputs)),\n",
        "        'std_throughput': float(np.std(throughputs)),\n",
        "        'memory_gb': memory_gb,\n",
        "        'params_b': model.num_parameters() / 1e9 if hasattr(model, 'num_parameters') else 0\n",
        "    }\n",
        "\n",
        "    print(f\"\\nðŸ“Š RESULTS:\")\n",
        "    print(f\"   â±ï¸  {results['mean_time']:.3f}s Â± {results['std_time']:.3f}s\")\n",
        "    print(f\"   ðŸš€ {results['median_throughput']:.1f} tok/s\")\n",
        "    print(f\"   ðŸ’¾ {results['memory_gb']:.2f} GB\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    ALL_RESULTS.append(results)\n",
        "    return results\n",
        "\n",
        "def clear_memory():\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"ðŸ—‘ï¸ Memory cleared\")\n",
        "\n",
        "print(\"âœ… Benchmark framework ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTHsksWwBeSo"
      },
      "source": [
        "---\n",
        "## MODEL 1: GPT-2 (124M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuMwJgEUBeSo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 1: GPT-2 (124M)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gpt2_id = \"gpt2\"\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(gpt2_id)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKnGzh4nBeSo"
      },
      "outputs": [],
      "source": [
        "# GPT-2 Baseline (FP32)\n",
        "print(\"\\n1ï¸âƒ£ GPT-2 Baseline (FP32)\")\n",
        "model_gpt2_base = AutoModelForCausalLM.from_pretrained(gpt2_id).to(\"cuda\")\n",
        "results_gpt2_base = benchmark_model(model_gpt2_base, tokenizer_gpt2, \"GPT-2\", \"Baseline (FP32)\")\n",
        "del model_gpt2_base\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmxGOYFHBeSo"
      },
      "outputs": [],
      "source": [
        "# GPT-2 FP16\n",
        "print(\"\\n2ï¸âƒ£ GPT-2 FP16\")\n",
        "model_gpt2_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    gpt2_id,\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "results_gpt2_fp16 = benchmark_model(model_gpt2_fp16, tokenizer_gpt2, \"GPT-2\", \"FP16\")\n",
        "del model_gpt2_fp16\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9m0Sj9TBeSp"
      },
      "outputs": [],
      "source": [
        "# GPT-2 INT8\n",
        "print(\"\\n3ï¸âƒ£ GPT-2 INT8\")\n",
        "model_gpt2_int8 = AutoModelForCausalLM.from_pretrained(\n",
        "    gpt2_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "results_gpt2_int8 = benchmark_model(model_gpt2_int8, tokenizer_gpt2, \"GPT-2\", \"INT8\")\n",
        "del model_gpt2_int8\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax74ITJeBeSp"
      },
      "source": [
        "---\n",
        "## MODEL 2: MISTRAL-7B (7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJXGanZGBeSp"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 2: MISTRAL-7B (7B)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mistral_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer_mistral = AutoTokenizer.from_pretrained(mistral_id)\n",
        "\n",
        "print(\"\\nðŸ“¥ Loading Mistral-7B (first time: ~5 min)...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UTRfRzBBeSp"
      },
      "outputs": [],
      "source": [
        "# Mistral Baseline\n",
        "print(\"\\n1ï¸âƒ£ Mistral-7B Baseline (FP16)\")\n",
        "model_mistral_base = AutoModelForCausalLM.from_pretrained(\n",
        "    mistral_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "results_mistral_base = benchmark_model(model_mistral_base, tokenizer_mistral, \"Mistral-7B\", \"Baseline (FP16)\")\n",
        "mistral_baseline = results_mistral_base['median_throughput']\n",
        "del model_mistral_base\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZCddlqeBeSp"
      },
      "outputs": [],
      "source": [
        "# Mistral FlashAttention-2\n",
        "print(\"\\n2ï¸âƒ£ Mistral-7B FlashAttention-2\")\n",
        "try:\n",
        "    model_mistral_flash = AutoModelForCausalLM.from_pretrained(\n",
        "        mistral_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation=\"flash_attention_2\"\n",
        "    )\n",
        "    results_mistral_flash = benchmark_model(model_mistral_flash, tokenizer_mistral, \"Mistral-7B\", \"FlashAttention-2\")\n",
        "    del model_mistral_flash\n",
        "except Exception as e:\n",
        "    print(f\"âŒ FlashAttention failed: {e}\")\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsDLl8pdBeSp"
      },
      "outputs": [],
      "source": [
        "# Mistral 4-bit\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "print(\"\\n3ï¸âƒ£ Mistral-7B 4-bit\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "model_mistral_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    mistral_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "results_mistral_4bit = benchmark_model(model_mistral_4bit, tokenizer_mistral, \"Mistral-7B\", \"4-bit (NF4)\")\n",
        "del model_mistral_4bit\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yng3KS2BeSp"
      },
      "source": [
        "---\n",
        "## MODEL 3: LLAMA-2-7B (7B) - Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o00BWIkfBeSp"
      },
      "outputs": [],
      "source": [
        "if LLAMA_AVAILABLE:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL 3: LLAMA-2-7B (7B)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    llama_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "    # Get token from environment\n",
        "    import os\n",
        "    from huggingface_hub import HfFolder\n",
        "\n",
        "    hf_token = os.environ.get('HF_TOKEN') or HfFolder.get_token()\n",
        "\n",
        "    if hf_token:\n",
        "        print(f\"ðŸ” Token: {hf_token[:15]}...\")\n",
        "    else:\n",
        "        print(\"âŒ No token!\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(llama_id, token=hf_token)\n",
        "    tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "    print(\"âœ… Llama-2 tokenizer loaded\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Skipping Llama-2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4XPn7oLBeSp"
      },
      "outputs": [],
      "source": [
        "if LLAMA_AVAILABLE:\n",
        "    # Llama Baseline\n",
        "    import os\n",
        "    hf_token = os.environ.get('HF_TOKEN')\n",
        "\n",
        "    print(\"\\n1ï¸âƒ£ Llama-2-7B Baseline (FP16)\")\n",
        "    model_llama_base = AutoModelForCausalLM.from_pretrained(\n",
        "        llama_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token\n",
        "    )\n",
        "    results_llama_base = benchmark_model(model_llama_base, tokenizer_llama, \"Llama-2-7B\", \"Baseline (FP16)\")\n",
        "    del model_llama_base\n",
        "    clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjrkXPesBeSp"
      },
      "outputs": [],
      "source": [
        "if LLAMA_AVAILABLE:\n",
        "    # Llama FlashAttention\n",
        "    import os\n",
        "    hf_token = os.environ.get('HF_TOKEN')\n",
        "\n",
        "    print(\"\\n2ï¸âƒ£ Llama-2-7B FlashAttention-2\")\n",
        "    try:\n",
        "        model_llama_flash = AutoModelForCausalLM.from_pretrained(\n",
        "            llama_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            attn_implementation=\"flash_attention_2\",\n",
        "            token=hf_token\n",
        "        )\n",
        "        results_llama_flash = benchmark_model(model_llama_flash, tokenizer_llama, \"Llama-2-7B\", \"FlashAttention-2\")\n",
        "        del model_llama_flash\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ FlashAttention failed: {e}\")\n",
        "    clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ39dMAbBeSp"
      },
      "outputs": [],
      "source": [
        "if LLAMA_AVAILABLE:\n",
        "    # Llama 4-bit\n",
        "    import os\n",
        "    hf_token = os.environ.get('HF_TOKEN')\n",
        "\n",
        "    print(\"\\n3ï¸âƒ£ Llama-2-7B 4-bit\")\n",
        "    model_llama_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "        llama_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token\n",
        "    )\n",
        "    results_llama_4bit = benchmark_model(model_llama_4bit, tokenizer_llama, \"Llama-2-7B\", \"4-bit (NF4)\")\n",
        "    del model_llama_4bit\n",
        "    clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXBDe83jBeSq"
      },
      "source": [
        "---\n",
        "## Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP_E922wBeSq"
      },
      "outputs": [],
      "source": [
        "# Results table\n",
        "df = pd.DataFrame(ALL_RESULTS)\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"ðŸŽ¯ COMPLETE RESULTS\")\n",
        "print(\"=\"*100)\n",
        "print()\n",
        "print(df[['model', 'config', 'median_throughput', 'memory_gb']].to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv('/content/benchmark_results.csv', index=False)\n",
        "print(\"\\nâœ… Saved: /content/benchmark_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOs-brM9BeSq"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Throughput\n",
        "pivot = df.pivot(index='config', columns='model', values='median_throughput')\n",
        "pivot.plot(kind='bar', ax=axes[0], width=0.8)\n",
        "axes[0].set_title('Throughput Comparison', fontsize=16, fontweight='bold')\n",
        "axes[0].set_ylabel('Tokens/Second', fontsize=13)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Memory\n",
        "pivot_mem = df.pivot(index='config', columns='model', values='memory_gb')\n",
        "pivot_mem.plot(kind='bar', ax=axes[1], width=0.8)\n",
        "axes[1].set_title('Memory Usage', fontsize=16, fontweight='bold')\n",
        "axes[1].set_ylabel('GPU Memory (GB)', fontsize=13)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/optimization_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Saved: /content/optimization_results.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xrUpIG4BeSq"
      },
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“ RESUME BULLETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "mistral_flash = df[(df['model'] == 'Mistral-7B') & (df['config'] == 'FlashAttention-2')]\n",
        "mistral_base = df[(df['model'] == 'Mistral-7B') & (df['config'].str.contains('Baseline'))]\n",
        "\n",
        "if len(mistral_flash) > 0 and len(mistral_base) > 0:\n",
        "    speedup = mistral_flash.iloc[0]['median_throughput'] / mistral_base.iloc[0]['median_throughput']\n",
        "    print(f\"\\nâ€¢ Accelerated Mistral-7B inference by {speedup:.1f}x through FlashAttention-2\")\n",
        "    print(f\"  CUDA optimization, reducing memory bandwidth bottlenecks\")\n",
        "\n",
        "print(f\"\\nâ€¢ Conducted multi-model optimization study across {df['model'].nunique()} models\")\n",
        "print(f\"  with {len(df)} configurations, demonstrating optimization scaling\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PROJECT COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip with all files\n",
        "!mkdir -p /content/project_files\n",
        "!cp /content/benchmark_results.csv /content/project_files/\n",
        "!cp /content/optimization_results.png /content/project_files/\n",
        "\n",
        "# Download the notebook\n",
        "# (You'll need to manually download the .ipynb file: File â†’ Download â†’ Download .ipynb)\n",
        "\n",
        "# Zip everything\n",
        "!cd /content && zip -r llm_optimization_project.zip project_files/\n",
        "\n",
        "print(\"âœ… Files packaged!\")\n",
        "print(\"ðŸ“¦ Downloading zip file...\")\n",
        "\n",
        "files.download('/content/llm_optimization_project.zip')"
      ],
      "metadata": {
        "id": "apCllhMQE978"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}